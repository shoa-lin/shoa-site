---
title: "揭秘 AI Agent 评估：从零到一的完整指南"
date: "2025-01-19"
source: "Anthropic Engineering"
sourceUrl: "https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents"
author: "Mikaela Grace, Jeremy Hadfield, Rodrigo Olivares, Jiri De Jonghe"
tags: ["AI", "Agent", "Evaluation", "Anthropic", "LLM"]
---

<style>
.blog-article-body {
    font-size: 1.05rem;
    line-height: 1.8;
}
.blog-article-body h2 {
    margin-top: 2.5rem;
    margin-bottom: 1.25rem;
    padding-bottom: 0.5rem;
    border-bottom: 2px solid #e5e7eb;
}
.blog-article-body h3 {
    margin-top: 2rem;
    margin-bottom: 1rem;
}
.blog-article-body h4 {
    margin-top: 1.5rem;
    margin-bottom: 0.75rem;
}
.blog-article-body p {
    margin-bottom: 1.25rem;
}
.blog-article-body ul, .blog-article-body ol {
    margin-bottom: 1.5rem;
}
.blog-article-body li {
    margin-bottom: 0.5rem;
}
.blog-article-body blockquote {
    border-left: 4px solid #667eea;
    padding-left: 1rem;
    margin: 1.5rem 0;
    color: #4b5563;
}
.blog-article-body code {
    background-color: #f3f4f6;
    padding: 0.125rem 0.375rem;
    border-radius: 0.25rem;
    font-size: 0.875em;
}
.blog-article-body pre {
    background-color: #1f2937;
    color: #e5e7eb;
    padding: 1rem;
    border-radius: 0.5rem;
    overflow-x: auto;
    margin: 1.5rem 0;
}
.blog-article-body pre code {
    background-color: transparent;
    padding: 0;
}
.blog-article-body img {
    max-width: 100%;
    height: auto;
    margin: 1.5rem 0;
    border-radius: 0.5rem;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}
@media (max-width: 768px) {
    .blog-article-body {
        font-size: 1rem;
    }
}
</style>

---

*发布于 2025年1月19日*
*原文：[Demystifying evals for AI agents](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents) | 作者：Mikaela Grace, Jeremy Hadfield, Rodrigo Olivares, Jiri De Jonghe*

---

## 引言

良好的评估（evaluations，简称 evals）帮助团队更有信心地发布 AI Agent。没有它们，团队很容易陷入被动循环——只在生产环境中发现问题，修复一个失败却造成另一个失败。评估在问题影响用户之前就让行为变化可见，其价值在 Agent 的整个生命周期中不断累积。

正如我们在《构建高效 Agent》中所述，Agent 通过多轮操作运行：调用工具、修改状态、根据中间结果进行适应。这些使 AI Agent 有用的能力——自主性、智能和灵活性——也让它们更难评估。

通过我们的内部工作以及与处于 Agent 发展前沿的客户合作，我们学会了为 Agent 设计更严谨、更有用的评估方法。以下是在真实部署中适用于多种 Agent 架构和用例的有效方法。

## 评估的结构

**评估（evaluation，简称 "eval"）**是对 AI 系统的测试：给 AI 一个输入，然后对其输出应用评分逻辑来衡量成功。在本文中，我们专注于可以在开发过程中运行的**自动化评估（automated evals）**，无需真实用户参与。

**单轮评估（single-turn evaluations）**很简单：一个提示、一个响应和评分逻辑。对于早期的 LLM，单轮、非 Agent 的评估是主要的评估方法。随着 AI 能力的进步，**多轮评估（multi-turn evaluations）**变得越来越普遍。

![单轮评估与多轮评估对比](https://www-cdn.anthropic.com/images/4zrzovbb/website/bd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834.png)

在简单的评估中，Agent 处理一个提示，评分器检查输出是否符合预期。对于更复杂的多轮评估，编码 Agent 接收工具、任务（在这种情况下是构建 MCP 服务器）和环境，执行"Agent 循环"（工具调用和推理），并使用实现更新环境。然后评分使用单元测试来验证工作的 MCP 服务器。

**Agent 评估**更加复杂。Agent 在多轮中使用工具，修改环境中的状态并随时适应——这意味着错误可以传播和复合。前沿模型还可以找到超越静态评估限制的创造性解决方案。例如，Opus 4.5 通过发现策略中的漏洞解决了 τ2-bench 中关于预订航班的问题。它在书面评估中"失败"了，但实际上为用户找到了更好的解决方案。

在构建 Agent 评估时，我们使用以下定义：

- **任务（task，也称 problem 或 test case）**：是具有定义输入和成功标准的单个测试
- **试验（trial）**：每次尝试任务是一次试验。由于模型输出在运行之间变化，我们运行多个试验以产生更一致的结果
- **评分器（grader）**：用于评分 Agent 性能某些方面的逻辑。一个任务可以有多个评分器，每个评分器包含多个断言（有时称为 **checks**）
- **轨录（transcript，也称 trace 或 trajectory）**：试验的完整记录，包括输出、工具调用、推理、中间结果以及任何其他交互。对于 Anthropic API，这是评估运行结束时的完整消息数组——包含评估期间对 API 的所有调用和所有返回的响应
- **结果（outcome）**：试验结束时环境的最终状态。航班预订 Agent 可能在轨录末尾说"您的航班已预订"，但结果是环境的 SQL 数据库中是否存在预订
- **评估框架（evaluation harness）**：端到端运行评估的基础设施。它提供说明和工具、并发运行任务、记录所有步骤、对输出进行评分并汇总结果
- **Agent 框架（agent harness，也称 scaffold）**：使模型能够充当 Agent 的系统：它处理输入、编排工具调用并返回结果。当我们评估"一个 Agent"时，我们是在评估框架**和**协同工作的模型。例如，Claude Code 是一个灵活的 Agent 框架，我们通过 Agent SDK 使用其核心原语来构建我们的长期运行 Agent 框架
- **评估套件（evaluation suite）**：旨在衡量特定能力或行为的任务集合。套件中的任务通常共享一个广泛的目标。例如，客户支持评估套件可能测试退款、取消和升级

![Agent 评估的组件](https://www-cdn.anthropic.com/images/4zrzovbb/website/0205b36f9639fc27f2f6566f73cb56b06f59d555-4584x2580.png)

## 为什么要构建评估？

当团队第一次开始构建 Agent 时，他们可以通过手动测试、内部使用（dogfooding）和直觉的组合取得惊人的进展。更严格的评估甚至可能被视为减缓发布的开销。但在早期的原型阶段之后，一旦 Agent 投入生产并开始扩展，没有评估的构建开始崩溃。

崩溃点通常出现在用户报告 Agent 在更改后感觉更差，团队"盲目飞行"，除了猜测和检查之外无法验证。没有评估，调试是被动的：等待投诉，手动重现，修复错误，希望没有其他回归。团队无法区分真正的回归和噪声，无法在发布前针对数百个场景自动测试更改，也无法衡量改进。

我们已经看到这种进展多次上演。例如，Claude Code 最初基于 Anthropic 员工和外部用户的反馈快速迭代。后来，我们添加了评估——首先是简洁性和文件编辑等狭窄领域，然后是过度工程等更复杂的行为。这些评估有助于识别问题，指导改进，并专注于研究-产品的合作。与生产监控、A/B 测试、用户研究等相结合，评估提供了继续改进 Claude Code 的信号。

在 Agent 生命周期的任何阶段编写评估都很有用。早期，评估迫使产品团队明确 Agent 的成功意味着什么，后来它们有助于保持一致的质量标准。

Descript 的 Agent 帮助用户编辑视频，因此他们围绕成功编辑工作流程的三个维度构建了评估：不破坏东西、做我要求的事、做得好。他们从手动评分发展到具有产品团队定义的标准和定期人工校准的 LLM 评分器，现在定期运行两个单独的套件用于质量基准测试和回归测试。Bolt AI 团队后来开始构建评估，当时他们已经有了一个广泛使用的 Agent。在 3 个月内，他们构建了一个评估系统，运行他们的 Agent 并使用静态分析对输出进行评分，使用浏览器 Agent 测试应用程序，并采用 LLM 评判来处理指令遵循等行为。

有些团队在开发开始时就创建评估；其他团队则在达到规模后添加评估，这时评估成为改进 Agent 的瓶颈。评估在 Agent 开发开始时特别有用，可以明确编码预期行为。两个工程师阅读相同的初始规范可能会对 AI 应该如何处理边缘情况得出不同的解释。评估套件解决了这种歧义。无论何时创建，评估都有助于加速开发。

评估还会影响您采用新模型的速度。当更强大的模型问世时，没有评估的团队面临数周的测试，而拥有评估的竞争对手可以快速确定模型的优势，调整提示，并在几天内升级。

一旦评估存在，您就可以免费获得基线和回归测试：延迟、令牌使用、每任务成本和错误率可以在静态任务库上跟踪。评估还可以成为产品团队和研究团队之间最高带宽的沟通渠道，定义研究人员可以优化的指标。显然，评估具有超出跟踪回归和改进的广泛好处。它们的累积价值很容易被忽视，因为成本前期可见，而效益后来累积。

## 如何评估 AI Agents

我们看到今天大规模部署的几种常见类型的 Agent：编码 Agent、研究 Agent、计算机使用 Agent 和对话 Agent。每种类型都可以在广泛的行业中部署，但可以使用类似技术进行评估。您不需要从头开始发明评估。以下部分描述了几种 Agent 类型的经过验证的技术。将这些方法作为基础，然后将其扩展到您的领域。

### Agent 的评分器类型

Agent 评估通常结合三种类型的评分器：基于代码、基于模型和基于人工。每个评分器评估轨录或结果的一部分。有效评估设计的一个基本组成部分是为工作选择合适的评分器。

**基于代码的评分器**

| 方法 | 优势 | 劣势 |
| --- | --- | --- |
| • 字符串匹配检查（精确、正则、模糊等）<br>• 二元测试（fail-to-pass、pass-to-pass）<br>• 静态分析（lint、类型、安全）<br>• 结果验证<br>• 工具调用验证（使用的工具、参数）<br>• 轨录分析（轮次、令牌使用） | • 快速<br>• 便宜<br>• 客观<br>• 可重现<br>• 易于调试<br>• 验证特定条件 | • 对不完全符合预期模式的有效变体很脆弱<br>• 缺乏细微差别<br>• 对于评估一些更主观的任务有限 |

**基于模型的评分器**

| 方法 | 优势 | 劣势 |
| --- | --- | --- |
| • 基于标准的评分<br>• 自然语言断言<br>• 成对比较<br>• 基于参考的评估<br>• 多评判共识 | • 灵活<br>• 可扩展<br>• 捕获细微差别<br>• 处理开放式任务<br>• 处理自由形式输出 | • 不确定性<br>• 比代码更昂贵<br>• 需要与人工评分器校准以获得准确性 |

**人工评分器**

| 方法 | 优势 | 劣势 |
| --- | --- | --- |
| • SME 审查<br>• 众包判断<br>• 抽查采样<br>• A/B 测试<br>• 评分者间一致性 | • 黄金标准质量<br>• 匹配专家用户判断<br>• 用于校准基于模型的评分器 | • 昂贵<br>• 慢<br>• 通常需要大规模访问人工专家 |

对于每个任务，评分可以是加权的（组合评分器分数必须达到阈值）、二元的（所有评分器必须通过）或混合的。

### 能力 vs 回归评估

**能力或"质量"评估**：询问"这个 Agent 能做什么好？"应该以低通过率开始，针对 Agent 困难的任务，给团队一个攀登的山坡。

**回归评估**：询问"Agent 是否仍然处理它以前处理的所有任务？"应该具有接近 100% 的通过率。它们防止倒退，因为得分下降表明有些东西坏了需要改进。当团队在能力评估上山爬时，同时运行回归评估以确保更改不会在其他地方引起问题也很重要。

在 Agent 启动和优化后，高通过率的能力评估可以"毕业"成为持续运行的回归套件，以捕获任何漂移。曾经测量"我们根本做不到这个？"的任务然后测量"我们仍然能可靠地做到这一点吗？"

### 评估编码 Agents

**编码 Agents**编写、测试和调试代码，导航代码库，并像人类开发人员一样运行命令。现代编码 Agent 的有效评估通常依赖于明确指定的任务、稳定的测试环境和生成代码的彻底测试。

确定性评分器对于编码 Agent 来说很自然，因为软件通常很容易评估：代码是否运行，测试是否通过？两个广泛使用的编码 Agent 基准测试，SWE-bench Verified 和 Terminal-Bench，遵循这种方法。SWE-bench Verified 给 Agent 提供来自流行 Python 存储库的 GitHub 问题，并通过运行测试套件来评分解决方案；只有在不破坏现有测试的情况下修复失败测试的解决方案才能通过。仅一年时间，LLM 在这个评估上就从 40% 进步到 >80%。Terminal-Bench 采用了不同的轨道：它测试端到端的技术任务，例如从源代码构建 Linux 内核或训练 ML 模型。

一旦您有一组用于验证编码任务关键**结果**的通过/失败测试，通常也有助于评估轨录**。**例如，基于启发式的代码质量规则可以不仅基于通过测试来评估生成的代码，具有清晰标准的基于模型的评分器可以评估 Agent 如何调用工具或与用户交互等行为。

**示例：编码 Agent 的理论评估**

考虑一个编码任务，其中 Agent 必须修复身份验证绕过漏洞。如下的 YAML 文件所示，可以使用评分器和指标来评估此 Agent。

```yaml
task:
  id: "fix-auth-bypass_1"
  desc: "修复密码字段为空时的身份验证绕过..."
  graders:
    - type: deterministic_tests
      required: [test_empty_pw_rejected.py, test_null_pw_rejected.py]
    - type: llm_rubric
      rubric: prompts/code_quality.md
    - type: static_analysis
      commands: [ruff, mypy, bandit]
    - type: state_check
      expect:
        security_logs: {event_type: "auth_blocked"}
    - type: tool_calls
      required:
        - {tool: read_file, params: {path: "src/auth/*"}}
        - {tool: edit_file}
        - {tool: run_tests}
  tracked_metrics:
    - type: transcript
      metrics:
        - n_turns
        - n_toolcalls
        - n_total_tokens
    - type: latency
      metrics:
        - time_to_first_token
        - output_tokens_per_sec
        - time_to_last_token
```

请注意，此示例展示了可用评分器的全部范围以进行说明。实际上，编码评估通常依赖单元测试进行正确性验证，并依赖 LLM 标准来评估整体代码质量，仅在需要时添加额外的评分器和指标。

### 评估对话 Agents

**对话 Agents**在支持、销售或教练等领域与用户互动。与传统聊天机器人不同，它们保持状态、使用工具并在对话中间采取行动。虽然编码和研究 Agent 也可以涉及与用户的多轮交互，但对话 Agent 呈现了一个独特的挑战：交互本身的质量是您评估的一部分。对话 Agent 的有效评估通常依赖于可验证的最终状态结果和捕获任务完成和交互质量的标准。与大多数其他评估不同，它们通常需要第二个 LLM 来模拟用户。我们在我们的对齐审计 Agent 中使用这种方法，通过扩展的对抗性对话来压力测试模型。

对话 Agent 的成功可能是多维度的：票据是否解决（状态检查），是否在 <10 轮内完成（轨录约束），语气是否合适（LLM 标准）？两个结合多维度的基准测试是 τ-Bench 及其继任者 τ2-Bench。这些模拟零售支持和航班预订等领域的多轮交互，其中一个模型扮演用户角色，而 Agent 导航现实场景。

**示例：对话 Agent 的理论评估**

考虑一个支持任务，其中 Agent 必须为沮丧的客户处理退款。

```yaml
graders:
  - type: llm_rubric
    rubric: prompts/support_quality.md
    assertions:
      - "Agent 对客户的挫折感表现出同理心"
      - "解决方案得到清楚解释"
      - "Agent 的回应基于 fetch_policy 工具结果"
  - type: state_check
    expect:
      tickets: {status: resolved}
      refunds: {status: processed}
  - type: tool_calls
    required:
      - {tool: verify_identity}
      - {tool: process_refund, params: {amount: "<=100"}}
      - {tool: send_confirmation}
  - type: transcript
    max_turns: 10
tracked_metrics:
  - type: transcript
    metrics:
      - n_turns
      - n_toolcalls
      - n_total_tokens
  - type: latency
    metrics:
      - time_to_first_token
      - output_tokens_per_sec
      - time_to_last_token
```

与我们的编码 Agent 示例一样，此任务展示了多种评分器类型以进行说明。实际上，对话 Agent 评估通常使用基于模型的评分器来评估沟通质量和目标完成，因为许多任务（如回答问题）可能有多个"正确"解决方案。

### 评估研究 Agents

**研究 Agents**收集、综合和分析信息，然后产生答案或报告等输出。与编码 Agent 不同，其中单元测试提供二元通过/失败信号，研究质量只能相对于任务来判断。什么算作"全面"、"来源充分"甚至"正确"取决于上下文：市场扫描、收购尽职调查和科学报告各自需要不同的标准。

研究评估面临独特的挑战：专家可能对综合是否全面存在分歧，基本事实随着参考内容不断变化而变化，更长、更开放的输出为错误创造了更多空间。例如 BrowseComp 这样的基准测试测试 AI Agent 是否可以在开放网络上寻找大海捞针——旨在易于验证但难以解决的问题。

构建研究 Agent 评估的一种策略是结合评分器类型。基础性检查验证声明得到检索来源的支持，覆盖检查定义良好答案必须包含的关键事实，源质量检查确认咨询的来源是权威的，而不仅仅是第一个检索的来源。对于具有客观正确答案的任务（"X 公司第三季度的收入是多少？"），精确匹配有效。LLM 可以标记无支持的声明和覆盖范围的缺口，还可以验证开放式的综合是否连贯和完整。

鉴于研究质量的主观性质，基于 LLM 的标准应该经常与专家人工判断校准，以有效评估这些 Agent。

### 计算机使用 Agents

**计算机使用 Agents**通过与人相同的界面与软件交互——屏幕截图、鼠标点击、键盘输入和滚动——而不是通过 API 或代码执行。它们可以使用任何具有图形用户界面（GUI）的应用程序，从设计工具到遗留企业软件。评估需要在真实或沙盒环境中运行 Agent，其中它可以使用软件应用程序，并检查它是否实现了预期结果。例如，WebArena 测试基于浏览器的任务，使用 URL 和页面状态检查来验证 Agent 是否正确导航，以及后端状态验证，用于修改数据的任务（确认订单实际上已放置，而不仅仅是确认页面出现）。OSWorld 将此扩展到完整的操作系统控制，评估脚本在任务完成后检查各种工件：文件系统状态、应用程序配置、数据库内容和 UI 元素属性。

浏览器使用 Agent 需要在令牌效率和延迟之间取得平衡。基于 DOM 的交互执行速度快但消耗大量令牌，而基于屏幕截图的交互较慢但更令牌高效。例如，当要求 Claude 总结 Wikipedia 时，从 DOM 中提取文本更高效。当在 Amazon 上寻找新的笔记本电脑外壳时，拍摄屏幕截图更有效（因为提取整个 DOM 是令牌密集型的）。在我们的 Claude for Chrome 产品中，我们开发了评估，以检查 Agent 是否为每个上下文选择了正确的工具。这使我们能够更快、更准确地完成基于浏览器的任务。

### 如何思考 Agent 评估中的非确定性

无论 Agent 类型如何，Agent 行为在运行之间都会变化，这使得评估结果比最初看起来更难解释。每个任务都有自己的成功率——也许一个任务 90%，另一个 50%——在一次评估运行中通过的任务可能在下一次失败。有时，我们要衡量的是 Agent 在**多少次**（试验的什么比例）试验中成功完成某项任务。

两个指标有助于捕捉这种细微差别：

**pass@k**衡量 Agent 在 **k** 次尝试中至少获得一个正确解决方案的可能性。随着 k 的增加，pass@k 分数上升——更多的"射门"意味着至少 1 次成功的几率更高。50% pass@1 的分数意味着模型在第一次尝试中成功完成评估中的一半任务。在编码中，我们通常最感兴趣的是 Agent 第一次尝试就找到解决方案——pass@1。在其他情况下，提出许多解决方案是有效的，只要有一个有效。

**pass^k**衡量所有 **k** 次试验都成功的概率。随着 **k** 的增加，pass^k 下降，因为在更多试验中要求一致性是一个更难清除的障碍。如果您的 Agent 每次试验的成功率为 75%，并且您运行 3 次试验，则三次试验都通过的概率是 (0.75)³ ≈ 42%。此指标对于面向客户的 Agent 尤其重要，用户每次都期望可靠的行为。

![pass@k 和 pass^k 随试验增加而分歧](https://www-cdn.anthropic.com/images/4zrzovbb/website/3ddac5be07a0773922ec9df06afec55922f8194a-4584x2580.png)

pass@k 和 pass^k 随着试验增加而分歧。在 k=1 时，它们是相同的（都等于每次试验的成功率）。到 k=10 时，它们讲述了相反的故事：pass@k 接近 100%，而 pass^k 下降到 0%。

两个指标都很有用，使用哪个取决于产品要求：pass@k 用于一次成功很重要的工具，pass^k 用于一致性至关重要的 Agent。

## 从零到一：构建优秀 Agent 评估的路线图

本节介绍了我们从无评估到可以信任的评估的实用、经过现场验证的建议。将此视为评估驱动的 Agent 开发的路线图：尽早定义成功，清楚地衡量它，并持续迭代。

### 收集初始评估数据集的任务

**步骤 0. 尽早开始**

我们看到团队因为认为他们需要数百个任务而推迟构建评估。实际上，从真实失败中得出的 20-50 个简单任务是一个很好的开始。毕竟，在早期 Agent 开发中，系统通常的每次更改都有明显、可见的影响，这种大效应大小意味着小样本量就足够了。更成熟的 Agent 可能需要更大、更难的评估来检测较小的效果，但在开始时最好采用 80/20 方法。等待时间越长，评估就越难构建。早期，产品需求自然会转化为测试用例。等待太久，您就会从实时系统逆向工程成功标准。

**步骤 1. 从您已经手动测试的内容开始**

从您在开发期间运行的手动检查开始——您在每次发布之前验证的行为和最终用户尝试的常见任务。如果您已经投入生产，请查看您的错误跟踪器和支持队列。将用户报告的失败转换为测试用例可确保您的套件反映实际使用；按用户影响进行优先级排序有助于您在重要的地方投资精力。

**步骤 2. 编写明确的任务和参考解决方案**

正确获取任务质量比看起来更难。一个好的任务是两个领域专家会独立得出相同的通过/失败 verdict 的任务。他们自己能通过任务吗？如果不能，任务需要完善。任务规范中的歧义会成为指标中的噪声。这同样适用于基于模型的评分器的标准：模糊的标准会产生不一致的判断。

每个任务都应该可以被正确遵循指令的 Agent 通过。这可能是微妙的。例如，审计 Terminal-Bench 发现，如果任务要求 Agent 编写脚本但没有指定文件路径，而测试假定脚本的特定文件路径，Agent 可能会因为自己的过错而失败。评分器检查的所有内容都应该从任务描述中清楚看出；Agent 不应该因为模糊的规范而失败。对于前沿模型，多次试验的 0% 通过率（即 0% pass@100）通常是任务损坏的信号，而不是无能的 Agent，这是双重检查任务规范和评分器的标志。对于每个任务，创建参考解决方案很有用：一个通过所有评分器的已知工作输出。这证明了任务是可解决的，并验证评分器配置正确。

**步骤 3. 构建平衡的问题集**

测试行为**应该**发生和**不应该**发生的情况。单方面的评估创建单方面的优化。例如，如果您只测试 Agent 是否在应该的时候搜索，您最终可能会得到一个搜索几乎所有内容的 Agent。尽量避免类别不平衡的评估。

我们在为 Claude.ai 中的网络搜索构建评估时亲自学到了这一点。挑战是防止模型在不应该搜索时进行搜索，同时保持它在适当的时候进行广泛研究的能力。团队构建了涵盖两个方向的评估：模型应该搜索的查询（如查找天气）和模型应该从现有知识回答的查询（如"谁创立了 Apple？"）。在触发不足（应该搜索时不搜索）或触发过度（不应该搜索时搜索）之间取得正确的平衡是很困难的，并且需要对提示和评估进行多轮改进。随着更多示例问题的出现，我们继续添加到评估中以改进我们的覆盖范围。

### 设计评估框架和评分器

**步骤 4：构建具有稳定环境的强大评估框架**

评估中的 Agent 的功能大致与生产中使用的 Agent 相同，环境本身不会引入进一步的噪声，这一点至关重要。每次试验都应该通过从干净的环境开始来"隔离"。运行之间不必要的共享状态（剩余文件、缓存数据、资源耗尽）可能导致由于基础设施波动而不是 Agent 性能导致的相关失败。共享状态也可能人为地提高性能。例如，在一些内部评估中，我们观察到 Claude 通过检查以前试验的 git 历史记录在某些任务上获得了不公平的优势。如果多个不同的试验由于环境中的相同限制（如有限的 CPU 内存）而失败，则这些试验不是独立的，因为它们受相同因素影响，评估结果对于测量 Agent 性能变得不可靠。

**步骤 5：深思熟虑地设计评分器**

如上所述，出色的评估设计涉及为 Agent 和任务选择最佳评分器。我们建议尽可能选择确定性评分器，在必要或额外灵活性时选择 LLM 评分器，并明智地使用人工评分器进行额外验证。

有一种常见的本能是检查 Agent 是否遵循了非常具体的步骤，比如按正确顺序的一系列工具调用。我们发现这种方法太僵化，导致测试过于脆弱，因为 Agent 经常找到评估设计者没有预料到的有效方法。为了不必要地惩罚创造力，通常最好评估 Agent 产生了什么，而不是它采取的路径。

对于具有多个组件的任务，建立部分信用**。**正确识别问题并验证客户但未能处理退款的支持 Agent 比立即失败的 Agent 有意义地更好。重要的是要在结果中表示这种连续的成功。

模型评分通常需要仔细迭代以验证准确性。LLM 作为评判评分器应与人类专家密切校准，以获得信心，即人工评分和模型评分之间几乎没有分歧。为了避免幻觉，给 LLM 一条出路，比如提供一条指令，在它没有足够信息时返回"未知"。创建清晰、结构化的标准来评估任务的每个维度，然后使用孤立的 LLM 作为评判来评估每个维度，而不是使用一个来评估所有维度，这也是很有帮助的。一旦系统健壮，偶尔使用人工审查就足够了。

一些评估有微妙的失败模式，即使 Agent 性能良好，也会导致低分，因为 Agent 由于评分错误、Agent 框架约束或歧义而无法解决任务。即使是复杂的团队也可能错过这些问题。例如，Opus 4.5 最初在 CORE-Bench 上得分为 42%，直到 Anthropic 研究人员发现了多个问题：僵化的评分，在期望"96.124991..."时惩罚"96.12"，模糊的任务规范，以及不可能完全重现的随机任务。修复错误并使用较少约束的框架后，Opus 4.5 的得分跃升至 95%。同样，METR 在其时间范围基准测试中发现了几个错误配置的任务，要求 Agent 优化到所述的分数阈值，但评分要求超过该阈值。这惩罚了像 Claude 这样遵循指令的模型，而忽略所述目标的模型获得更好的分数。仔细双重检查任务和评分器可以帮助避免这些问题。

使您的评分器能够抵抗绕过或黑客攻击。Agent 不应该能够轻易"作弊"评估。任务和评分器的设计应该是通过真正需要解决问题，而不是利用意外的漏洞。

### 长期维护和使用评估

**步骤 6：检查轨录**

除非您阅读来自多次试验的轨录和评分，否则您不会知道您的评分器是否工作良好。在 Anthropic，我们投资了用于查看评估轨录的工具，我们定期花时间阅读它们。当任务失败时，轨录告诉您 Agent 是犯了真正的错误还是您的评分器拒绝了一个有效的解决方案。它通常还会显示有关 Agent 和评估行为的关键细节。

失败应该是公平的：很清楚 Agent 做错了什么以及为什么。当分数不上升时，我们需要信心这是由于 Agent 性能，而不是评估。阅读轨录是您验证评估是否衡量真正重要的事情的方式，这是 Agent 开发的一项关键技能。

**步骤 7：监控能力评估饱和**

100% 的评估跟踪回归，但不提供改进的信号。当 Agent 通过所有可解决的任务时，**评估饱和**就会发生，没有改进的空间。例如，SWE-Bench Verified 的分数今年从 30% 开始，前沿模型现在接近 >80% 的饱和度。随着评估接近饱和，进展也会放缓，因为只有最困难的任务仍然存在。这可能会使结果具有欺骗性，因为大的能力改进显示为分数的小幅增加。例如，代码审查初创公司 Qodo 最初对 Opus 4.5 印象不深刻，因为他们的单次编码评估没有捕捉到在更长、更复杂的任务上的收益。作为回应，他们开发了一个新的 Agent 评估框架，提供了更清晰的进展图。

作为规则，在我们有人深入挖掘评估的细节并阅读一些轨录之前，我们不会按面值接受评估分数。如果评分不公平、任务模糊、有效解决方案受到惩罚，或者框架限制了模型，则应修改评估。

**步骤 8：通过开放贡献和维护长期保持评估套件健康**

评估套件是一个活的人工制品，需要持续的关注和明确的所有权才能保持有用。

在 Anthropic，我们尝试了各种评估维护方法。最有效的方法是建立专门的评估团队来拥有核心基础设施，而领域专家和产品团队贡献大多数评估任务并自己运行评估。

对于 AI 产品团队，拥有和迭代评估应该像维护单元测试一样例行公事。团队可能会在"工作"的早期测试但在 AI 功能上浪费数周，但未能满足精心设计的评估会早期揭示的未说明期望。定义评估任务是压力测试产品需求是否足够具体以开始构建的最佳方法之一。

我们建议练习评估驱动的开发：在 Agent 能够满足它们之前构建评估来定义计划的能力，然后迭代直到 Agent 表现良好。在内部，我们经常构建今天"足够好"的功能，但押注模型在几个月内能做什么。从低通过率开始的能力评估使这可见。当新模型问世时，运行套件快速揭示了哪些赌注得到了回报。

最接近产品要求和用户的人最适合定义成功。凭借当前的模型能力，产品经理、客户成功经理或销售人员可以使用 Claude Code 贡献评估任务作为 PR——让他们做吧！或者甚至更好，积极地启用他们。

![创建有效评估的过程](https://www-cdn.anthropic.com/images/4zrzovbb/website/0db40cc0e14402222a179fc6297b9c8818e97c8a-4584x2580.png)

## 评估与其他方法的结合

可以对 Agent 在数千个任务中运行自动化评估，而无需部署到生产或影响真实用户。但这只是了解 Agent 性能的多种方式之一。完整的画面包括生产监控、用户反馈、A/B 测试、手动轨录审查和系统性人类评估。

了解 AI Agent 性能的方法概述

| 方法 | 优点 | 缺点 |
| --- | --- | --- |
| **自动化评估**<br>在没有真实用户的情况下以编程方式运行测试 | • 更快的迭代<br>• 完全可重现<br>• 无用户影响<br>• 可以在每次提交时运行<br>• 在不需要生产部署的情况下大规模测试场景 | • 需要更多前期投资来构建<br>• 需要持续维护，因为产品和模型演变以避免漂移<br>• 如果与真实使用模式不匹配，可能会产生虚假的信心 |
| **生产监控**<br>跟踪实时系统中的指标和错误 | • 在规模上揭示真实用户行为<br>• 捕获合成评估遗漏的问题<br>• 提供有关 Agent 实际表现的基本事实 | • 被动，问题在您知道之前就到达了用户<br>• 信号可能很嘈杂<br>• 需要投资仪器<br>• 缺乏评分的基本事实 |
| **A/B 测试**<br>与真实用户流量比较变体 | • 衡量实际用户结果（保留、任务完成）<br>• 控制混淆<br>• 可扩展和系统化 | • 慢，需要数天或数周才能达到意义并需要足够的流量<br>• 仅测试您部署的更改<br>• 在指标中变化的潜在"为什么"上信号较少，而无法彻底审查轨录 |
| **用户反馈**<br>明确的信号，如竖起大拇指或错误报告 | • 揭示您没有预料到的问题<br>• 来自实际人类用户的真实示例<br>• 反馈通常与产品目标相关 | • 稀疏和自我选择<br>• 倾向于严重问题<br>• 用户很少解释**为什么**失败了<br>• 不自动化<br>• 主要依靠用户来捕捉问题可能会产生负面影响用户影响 |
| **手动轨录审查**<br>人类阅读 Agent 对话 | • 建立对失败模式的直觉<br>• 捕获自动检查遗漏的微妙质量问题<br>• 有助于校准"好"的样子并掌握细节 | • 时间密集<br>• 不扩展<br>• 覆盖不一致<br>• 审查者疲劳或不同的审查者可能会影响信号质量<br>• 通常只给出定性信号，而不是明确的定量评分 |
| **系统性人类研究**<br>训练评分员对 Agent 输出的结构化评分 | • 来自多个人类评分员的黄金标准质量判断<br>• 处理主观或模糊的任务<br>• 为改进基于模型的评分器提供信号 | • 相对昂贵且周转慢<br>• 难以频繁运行<br>• 评分者间不一致需要和解<br>• 复杂领域（法律、金融、医疗保健）需要人类专家进行研究 |

这些方法映射到 Agent 开发的不同阶段。自动化评估在发布前和 CI/CD 中特别有用，在每次 Agent 更改和模型升级上运行，作为防止质量问题的第一道防线。生产监控在发布后启动，以检测分布漂移和意外现实世界的失败。A/B 测试验证重大更改，一旦您有足够的流量。用户反馈和轨录审查是持续的实践，以填补空白 - 不断分类反馈，每周抽样轨录阅读，并根据需要更深入地挖掘。保留系统性人类研究，用于校准 LLM 评分器或评估主观输出，其中人类共识作为参考标准。

![了解 AI Agent 性能的方法对比](https://www-cdn.anthropic.com/images/4zrzovbb/website/b77b8dbb7c2e57f063fbc8a087a853d5809b74b0-4584x2580.png)

就像安全工程中的瑞士奶酪模型一样，没有一个评估层能捕捉到每个问题。结合多种方法，遗漏一层的问题被另一层捕获。

最有效的团队结合这些方法 - 用于快速迭代的自动化评估，用于基本事实的生产监控，以及用于校准的定期人工审查。

## 结论

没有评估的团队陷入被动循环 - 修复一个失败，造成另一个，无法区分真正的回归和噪声。早期投资的团队发现相反：随着失败变成测试用例，测试用例防止回归，指标取代猜测，开发加速。评估给整个团队一个清晰的山坡要爬，将"Agent 感觉更差"变成可操作的东西。价值复合，但只有当您将评估视为核心组件，而不是事后诸葛亮。

模式因 Agent 类型而异，但这里描述的基础知识是不变的。尽早开始，不要等待完美的套件。从您看到的失败中收集现实的任务。定义明确、强大的成功标准。深思熟虑地设计评分器并结合多种类型。确保问题对模型来说足够难。迭代评估以提高其信噪比。阅读轨录！

AI Agent 评估仍然是一个新生、快速发展的领域。随着 Agent 承担更长的任务，在多 Agent 系统中协作，并处理越来越主观的工作，我们将需要适应我们的技术。我们将继续分享最佳实践，因为我们了解更多。

### 关键要点

1. **尽早开始**，不要等待完美的套件
2. **从您看到的失败中收集现实的任务**
3. **定义明确、强大的成功标准**
4. **深思熟虑地设计评分器并结合多种类型**
5. **确保问题对模型来说足够难**
6. **迭代评估以提高其信噪比**
7. **阅读轨录！**

### 致谢

由 Mikaela Grace、Jeremy Hadfield、Rodrigo Olivares 和 Jiri De Jonghe 撰写。我们还感谢 David Hershey、Gian Segato、Mike Merrill、Alex Shaw、Nicholas Carlini、Ethan Dixon、Pedram Navid、Jake Eaton、Alyssa Baum、Lina Tawfik、Karen Zhou、Alexander Bricken、Sam Kennedy、Robert Ying 以及其他人的贡献。特别感谢我们通过评估合作学习的客户和合作伙伴，包括 iGent、Cognition、Bolt、Sierra、Vals.ai、Macroscope、PromptLayer、Stripe、Shopify、Terminal Bench 团队等。这项工作反映了几个团队的集体努力，他们帮助开发了 Anthropic 的评估实践。

## 附录：评估框架

几个开源和商业框架可以帮助团队从零开始构建基础设施来实现 Agent 评估。正确的选择取决于您的 Agent 类型、现有堆栈以及您是否需要离线评估、生产可观察性或两者兼而有之。

**Harbor** 旨在在容器化环境中运行 Agent，具有跨云提供商大规模运行试验的基础设施，以及用于定义任务和评分器的标准化格式。像 Terminal-Bench 2.0 这样的流行基准测试通过 Harbor 注册表提供，使您可以轻松运行既定的基准测试以及自定义评估套件。

**Promptfoo** 是一个轻量级、灵活的开源框架，专注于用于提示测试的声明性 YAML 配置，断言类型从字符串匹配到 LLM 作为评判标准。我们对许多产品评估使用 Promptfoo 的版本。

**Braintrust** 是一个结合离线评估与生产可观察性和实验跟踪的平台 - 对于需要在开发期间迭代并在生产中监控质量的团队很有用。它的 `autoevals` 库包括针对事实性、相关性和其他常见维度的预构建评分器。

**LangSmith** 提供跟踪、离线和在线评估以及数据集管理，与 LangChain 生态系统紧密集成。**Langfuse** 提供类似的功能作为自我托管的开源替代方案，适用于具有数据驻留要求的团队。

许多团队结合多个工具，滚动自己的评估框架，或者只是使用简单的评估脚本作为起点。我们发现，虽然框架可以是加速进展和标准化的有价值的方式，但它们只有在您通过它们运行的评估任务方面才有效。通常最好快速选择一个适合您工作流程的框架，然后通过迭代高质量的测试用例和评分器来投资评估本身。

---
